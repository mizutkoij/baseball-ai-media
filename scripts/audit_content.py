#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç›£æŸ»ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ç¬¬ä¸‰è€…ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ··å…¥ã‚’n-gram/é¡žä¼¼åº¦ã§ãƒã‚§ãƒƒã‚¯
"""

import os
import re
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Tuple, Set
from collections import Counter
import difflib
from dataclasses import dataclass

@dataclass
class ContentMatch:
    file_path: str
    line_number: int
    content: str
    similarity: float
    matched_pattern: str
    severity: str  # 'high', 'medium', 'low'

class ContentAuditor:
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç›£æŸ»ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.blocked_patterns = self._load_blocked_patterns()
        self.similarity_threshold = 0.8  # 80%ä»¥ä¸Šã§è­¦å‘Š
        
    def _load_blocked_patterns(self) -> List[str]:
        """ãƒ–ãƒ­ãƒƒã‚¯å¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®èª­ã¿è¾¼ã¿"""
        # 1point02ç”±æ¥ã®æ—¢çŸ¥ãƒ•ãƒ¬ãƒ¼ã‚ºãƒ»è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns = [
            # ã‚µã‚¤ãƒˆåãƒ»URL
            "1point02",
            "1point02.jp",
            "ãƒ¯ãƒ³ãƒã‚¤ãƒ³ãƒˆã‚¼ãƒ­ãƒ„ãƒ¼",
            
            # ç‰¹æœ‰ã®è¡¨ç¾ãƒ»ç”¨èªž
            "çƒè©³",
            "çƒè©³ãƒ‡ãƒ¼ã‚¿",
            "ã‚»ã‚¤ãƒãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©³ç´°",
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç‰¹æœ‰ã®æŒ‡æ¨™åãƒ»ç•¥ç§°
            "RCAA",
            "WAR(rWAR)",
            "rWAR",
            "wSB",
            "UZR/150",
            
            # ç‰¹å¾´çš„ãªæ–‡ç« ãƒ‘ã‚¿ãƒ¼ãƒ³
            "æ‰“è€…ã‚’ç›¸æ‰‹ã«ã—ãŸéš›ã®",
            "æŠ•æ‰‹ã«å¯¾ã™ã‚‹",
            "ãƒ¬ãƒãƒ¬ãƒƒã‚¸æŒ‡æ•°",
            "çŠ¶æ³åˆ¥æˆç¸¾",
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®è¡¨ç¾
            "å¹´åº¦åˆ¥æˆç¸¾",
            "æœˆåˆ¥æˆç¸¾", 
            "å¯¾æˆ¦ç›¸æ‰‹åˆ¥",
            "ã‚¤ãƒ‹ãƒ³ã‚°åˆ¥",
            
            # ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒ»è‘—ä½œæ¨©é–¢é€£
            "1point02.jp All Rights Reserved",
            "æ ªå¼ä¼šç¤¾DELTA",
            "DELTA:GRAPH",
            
            # APIãƒ»ãƒ‡ãƒ¼ã‚¿å½¢å¼ç‰¹æœ‰
            "player_id",
            "team_id", 
            "game_id",
            # ãŸã ã—ã€ã“ã‚Œã‚‰ã¯ä¸€èˆ¬çš„ã™ãŽã‚‹ã®ã§é™¤å¤–å€™è£œ
        ]
        
        return patterns
    
    def _extract_ngrams(self, text: str, n: int = 3) -> Set[str]:
        """n-gramæŠ½å‡º"""
        # æ—¥æœ¬èªžå¯¾å¿œã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        # å®Ÿéš›ã«ã¯MeCabç­‰ã‚’ä½¿ã†ã¨ã‚ˆã‚Šç²¾å¯†
        tokens = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾¯a-zA-Z0-9]+', text)
        
        ngrams = set()
        for i in range(len(tokens) - n + 1):
            ngram = ''.join(tokens[i:i+n])
            ngrams.add(ngram)
        
        return ngrams
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """æ–‡ç« é¡žä¼¼åº¦è¨ˆç®—ï¼ˆJaccardä¿‚æ•°ãƒ™ãƒ¼ã‚¹ï¼‰"""
        ngrams1 = self._extract_ngrams(text1)
        ngrams2 = self._extract_ngrams(text2)
        
        if not ngrams1 and not ngrams2:
            return 0.0
        if not ngrams1 or not ngrams2:
            return 0.0
            
        intersection = len(ngrams1 & ngrams2)
        union = len(ngrams1 | ngrams2)
        
        return intersection / union if union > 0 else 0.0
    
    def _check_direct_patterns(self, content: str) -> List[Tuple[str, str]]:
        """ç›´æŽ¥çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒžãƒƒãƒãƒ³ã‚°"""
        matches = []
        
        for pattern in self.blocked_patterns:
            if pattern.lower() in content.lower():
                matches.append((pattern, "direct_match"))
        
        return matches
    
    def _analyze_file_content(self, file_path: Path) -> List[ContentMatch]:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æž"""
        matches = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except (UnicodeDecodeError, PermissionError):
            # ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚„ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—
            return matches
        
        for line_num, line in enumerate(lines, 1):
            line_content = line.strip()
            if not line_content or line_content.startswith('//') or line_content.startswith('#'):
                continue
            
            # ç›´æŽ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒžãƒƒãƒ
            direct_matches = self._check_direct_patterns(line_content)
            for pattern, match_type in direct_matches:
                severity = "high" if "1point02" in pattern else "medium"
                matches.append(ContentMatch(
                    file_path=str(file_path.relative_to(self.project_root)),
                    line_number=line_num,
                    content=line_content,
                    similarity=1.0,  # ç›´æŽ¥ãƒžãƒƒãƒã¯100%
                    matched_pattern=pattern,
                    severity=severity
                ))
            
            # é¡žä¼¼åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆæ—¢çŸ¥ã®å•é¡Œæ–‡ç« ã¨æ¯”è¼ƒï¼‰
            known_phrases = [
                "ãƒãƒ¼ãƒ ã®å¾—ç‚¹ã¨å¤±ç‚¹ã‹ã‚‰æœŸå¾…å‹çŽ‡ã‚’ç®—å‡ºã™ã‚‹æŒ‡æ¨™",
                "æŠ•æ‰‹ã®è¢«å®‰æ‰“çŽ‡ã¯é‹ã®è¦ç´ ãŒå¤§ãã„",
                "ãƒ‘ãƒ¼ã‚¯ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã«ã‚ˆã‚‹çƒå ´è£œæ­£"
            ]
            
            for known_phrase in known_phrases:
                similarity = self._calculate_similarity(line_content, known_phrase)
                if similarity >= self.similarity_threshold:
                    matches.append(ContentMatch(
                        file_path=str(file_path.relative_to(self.project_root)),
                        line_number=line_num,
                        content=line_content,
                        similarity=similarity,
                        matched_pattern=f"Similar to: {known_phrase[:50]}...",
                        severity="medium"
                    ))
        
        return matches
    
    def audit_project(self) -> Dict:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ç›£æŸ»"""
        
        # å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­
        target_extensions = {'.tsx', '.ts', '.js', '.jsx', '.md', '.json', '.txt', '.py'}
        
        # é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        exclude_dirs = {'node_modules', '.git', '.next', 'dist', 'build', '__pycache__'}
        
        all_matches = []
        processed_files = 0
        
        for root, dirs, files in os.walk(self.project_root):
            # é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            
            for file in files:
                file_path = Path(root) / file
                
                # æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
                if file_path.suffix not in target_extensions:
                    continue
                
                matches = self._analyze_file_content(file_path)
                all_matches.extend(matches)
                processed_files += 1
        
        # çµæžœã®é›†è¨ˆ
        severity_counts = Counter(match.severity for match in all_matches)
        file_counts = Counter(match.file_path for match in all_matches)
        
        return {
            'total_files_processed': processed_files,
            'total_matches': len(all_matches),
            'severity_breakdown': dict(severity_counts),
            'affected_files': len(file_counts),
            'matches': [
                {
                    'file': match.file_path,
                    'line': match.line_number,
                    'content': match.content,
                    'similarity': round(match.similarity, 3),
                    'pattern': match.matched_pattern,
                    'severity': match.severity
                }
                for match in sorted(all_matches, key=lambda x: (x.severity == 'high', x.similarity), reverse=True)
            ]
        }
    
    def generate_report(self, results: Dict) -> str:
        """ç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = []
        report.append("# ðŸ“‹ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆ")
        report.append(f"**ç”Ÿæˆæ—¥æ™‚:** {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # ã‚µãƒžãƒªãƒ¼
        report.append("## ðŸ“Š ã‚µãƒžãƒªãƒ¼")
        report.append(f"- **å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°:** {results['total_files_processed']}")
        report.append(f"- **æ¤œå‡ºé …ç›®æ•°:** {results['total_matches']}")
        report.append(f"- **å½±éŸ¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°:** {results['affected_files']}")
        report.append("")
        
        # æ·±åˆ»åº¦åˆ¥
        report.append("## âš ï¸ æ·±åˆ»åº¦åˆ¥")
        for severity, count in results['severity_breakdown'].items():
            emoji = "ðŸš¨" if severity == "high" else "âš ï¸" if severity == "medium" else "â„¹ï¸"
            report.append(f"- **{emoji} {severity.upper()}:** {count}ä»¶")
        report.append("")
        
        # æ¤œå‡ºé …ç›®è©³ç´°
        if results['matches']:
            report.append("## ðŸ” æ¤œå‡ºé …ç›®")
            for match in results['matches'][:20]:  # ä¸Šä½20ä»¶
                emoji = "ðŸš¨" if match['severity'] == "high" else "âš ï¸" if match['severity'] == "medium" else "â„¹ï¸"
                report.append(f"### {emoji} {match['file']}:{match['line']}")
                report.append(f"**é¡žä¼¼åº¦:** {match['similarity']:.1%}")
                report.append(f"**ãƒ‘ã‚¿ãƒ¼ãƒ³:** {match['pattern']}")
                report.append(f"**å†…å®¹:** `{match['content'][:100]}...`")
                report.append("")
        else:
            report.append("## âœ… æ¤œå‡ºé …ç›®")
            report.append("å•é¡Œã¨ãªã‚‹é¡žä¼¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            report.append("")
        
        # æŽ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        if results['total_matches'] > 0:
            report.append("## ðŸ› ï¸ æŽ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
            if results['severity_breakdown'].get('high', 0) > 0:
                report.append("1. **HIGHå„ªå…ˆåº¦é …ç›®ã®å³åº§ä¿®æ­£**")
                report.append("2. è©²å½“ç®‡æ‰€ã‚’ã‚ªãƒªã‚¸ãƒŠãƒ«æ–‡ç« ã«ç½®ãæ›ãˆ")
                report.append("3. é¡žä¼¼ãƒ•ãƒ¬ãƒ¼ã‚ºã®å…¨æ–‡æ›¸è¦‹ç›´ã—")
            else:
                report.append("1. MEDIUM/LOWé …ç›®ã®æ®µéšŽçš„ä¿®æ­£")
                report.append("2. ç‹¬è‡ªè¡¨ç¾ã¸ã®è¨€ã„æ›ãˆæ¤œè¨Ž")
            report.append("")
        
        return "\n".join(report)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Content audit for copyright compliance")
    parser.add_argument("--project-root", default=".", help="Project root directory")
    parser.add_argument("--output", default="audit_report.md", help="Output report file")
    parser.add_argument("--ci", action="store_true", help="CI mode (exit with error if issues found)")
    
    args = parser.parse_args()
    
    auditor = ContentAuditor(args.project_root)
    print("Content audit starting...")
    
    results = auditor.audit_project()
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = auditor.generate_report(results)
    
    with open(args.output, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"Audit completed: {args.output}")
    print(f"   Files processed: {results['total_files_processed']}")
    print(f"   Issues detected: {results['total_matches']}")
    
    # CI ãƒ¢ãƒ¼ãƒ‰ã§ã®çµ‚äº†ã‚³ãƒ¼ãƒ‰
    if args.ci:
        high_severity = results['severity_breakdown'].get('high', 0)
        if high_severity > 0:
            print(f"CI FAIL: {high_severity} high priority issues")
            exit(1)
        else:
            print("CI PASS: No high priority issues")
            exit(0)

if __name__ == "__main__":
    main()