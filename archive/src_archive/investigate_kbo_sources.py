#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
investigate_kbo_sources.py
==========================
KBOãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®èª¿æŸ»ãƒ»åˆ†æ
"""

import requests
from bs4 import BeautifulSoup
import time
import json
from urllib.parse import urljoin, urlparse
import re

# KBOãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
SOURCES = {
    'mykbostats': 'https://mykbostats.com',
    'koreabaseball_en': 'https://eng.koreabaseball.com',
    'koreabaseball_kr': 'https://www.koreabaseball.com'
}

def investigate_site(name, base_url):
    """ã‚µã‚¤ãƒˆæ§‹é€ ã‚’èª¿æŸ»"""
    print(f"\n{'='*60}")
    print(f"èª¿æŸ»ä¸­: {name} ({base_url})")
    print(f"{'='*60}")
    
    try:
        # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
        response = requests.get(base_url, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # åŸºæœ¬æƒ…å ±
        title = soup.find('title')
        print(f"ã‚¿ã‚¤ãƒˆãƒ«: {title.get_text(strip=True) if title else 'ãªã—'}")
        
        # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ãƒ¡ãƒ‹ãƒ¥ãƒ¼æ§‹é€ ã‚’èª¿æŸ»
        nav_elements = soup.find_all(['nav', 'menu', 'ul'])
        
        # ãƒªãƒ³ã‚¯åé›†
        links = []
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            text = a_tag.get_text(strip=True)
            if text and len(text) < 100:  # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯é™¤å¤–
                full_url = urljoin(base_url, href)
                links.append({
                    'text': text,
                    'url': full_url,
                    'is_internal': urlparse(full_url).netloc == urlparse(base_url).netloc
                })
        
        # å†…éƒ¨ãƒªãƒ³ã‚¯ã®ã¿è¡¨ç¤º
        internal_links = [link for link in links if link['is_internal']]
        
        print(f"\nä¸»è¦ãªå†…éƒ¨ãƒªãƒ³ã‚¯ ({len(internal_links)}å€‹ä¸­æœ€åˆã®20å€‹):")
        for i, link in enumerate(internal_links[:20]):
            print(f"  {i+1:2d}. {link['text'][:50]} -> {link['url']}")
        
        # é¸æ‰‹ãƒ»ãƒãƒ¼ãƒ é–¢é€£ã®ãƒªãƒ³ã‚¯ã‚’ç‰¹å®š
        player_related = []
        team_related = []
        stats_related = []
        
        keywords_player = ['player', 'roster', 'ì„ ìˆ˜', 'ë¡œìŠ¤í„°', 'batter', 'pitcher']
        keywords_team = ['team', 'club', 'íŒ€', 'êµ¬ë‹¨']  
        keywords_stats = ['stats', 'statistics', 'ê¸°ë¡', 'í†µê³„', 'standings', 'ranking']
        
        for link in internal_links:
            text_lower = link['text'].lower()
            url_lower = link['url'].lower()
            
            if any(keyword in text_lower or keyword in url_lower for keyword in keywords_player):
                player_related.append(link)
            elif any(keyword in text_lower or keyword in url_lower for keyword in keywords_team):
                team_related.append(link)
            elif any(keyword in text_lower or keyword in url_lower for keyword in keywords_stats):
                stats_related.append(link)
        
        if player_related:
            print(f"\nğŸƒâ€â™‚ï¸ é¸æ‰‹é–¢é€£ãƒªãƒ³ã‚¯ ({len(player_related)}å€‹):")
            for link in player_related[:10]:
                print(f"  - {link['text']} -> {link['url']}")
        
        if team_related:
            print(f"\nğŸŸï¸ ãƒãƒ¼ãƒ é–¢é€£ãƒªãƒ³ã‚¯ ({len(team_related)}å€‹):")
            for link in team_related[:10]:
                print(f"  - {link['text']} -> {link['url']}")
                
        if stats_related:
            print(f"\nğŸ“Š çµ±è¨ˆé–¢é€£ãƒªãƒ³ã‚¯ ({len(stats_related)}å€‹):")
            for link in stats_related[:10]:
                print(f"  - {link['text']} -> {link['url']}")
        
        return {
            'title': title.get_text(strip=True) if title else None,
            'total_links': len(links),
            'internal_links': len(internal_links),
            'player_links': player_related,
            'team_links': team_related,
            'stats_links': stats_related,
            'sample_links': internal_links[:20]
        }
        
    except requests.RequestException as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return None
    except Exception as e:
        print(f"âŒ è§£æã‚¨ãƒ©ãƒ¼: {e}")
        return None

def investigate_specific_page(url, description=""):
    """ç‰¹å®šã®ãƒšãƒ¼ã‚¸ã‚’è©³ç´°èª¿æŸ»"""
    print(f"\nğŸ” è©³ç´°èª¿æŸ»: {description}")
    print(f"URL: {url}")
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚’èª¿æŸ»
        tables = soup.find_all('table')
        print(f"ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")
        
        for i, table in enumerate(tables[:3]):  # æœ€åˆã®3ã¤ã®ãƒ†ãƒ¼ãƒ–ãƒ«
            print(f"\nãƒ†ãƒ¼ãƒ–ãƒ« {i+1}:")
            headers = table.find_all(['th', 'td'])[:10]  # æœ€åˆã®10ã‚«ãƒ©ãƒ 
            if headers:
                header_texts = [h.get_text(strip=True) for h in headers]
                print(f"  ãƒ˜ãƒƒãƒ€ãƒ¼ä¾‹: {header_texts}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ 
        forms = soup.find_all('form')
        if forms:
            print(f"\nãƒ•ã‚©ãƒ¼ãƒ æ•°: {len(forms)}")
            for i, form in enumerate(forms[:2]):
                inputs = form.find_all(['input', 'select'])
                if inputs:
                    input_info = [(inp.get('name', 'unnamed'), inp.get('type', 'unknown')) for inp in inputs[:5]]
                    print(f"  ãƒ•ã‚©ãƒ¼ãƒ {i+1}ã®å…¥åŠ›é …ç›®: {input_info}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è©³ç´°èª¿æŸ»ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    print("KBO ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹èª¿æŸ»é–‹å§‹")
    print("="*60)
    
    results = {}
    
    # å„ã‚µã‚¤ãƒˆã‚’èª¿æŸ»
    for name, url in SOURCES.items():
        results[name] = investigate_site(name, url)
        time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
    
    # MyKBOstatsã®è©³ç´°èª¿æŸ»
    if results.get('mykbostats'):
        mykbo_data = results['mykbostats']
        
        # é¸æ‰‹é–¢é€£ãƒšãƒ¼ã‚¸ãŒã‚ã‚Œã°è©³ç´°èª¿æŸ»
        if mykbo_data and mykbo_data.get('player_links'):
            player_link = mykbo_data['player_links'][0]
            investigate_specific_page(player_link['url'], f"é¸æ‰‹ãƒšãƒ¼ã‚¸: {player_link['text']}")
            time.sleep(2)
        
        # ãƒãƒ¼ãƒ é–¢é€£ãƒšãƒ¼ã‚¸ãŒã‚ã‚Œã°è©³ç´°èª¿æŸ»
        if mykbo_data and mykbo_data.get('team_links'):
            team_link = mykbo_data['team_links'][0] 
            investigate_specific_page(team_link['url'], f"ãƒãƒ¼ãƒ ãƒšãƒ¼ã‚¸: {team_link['text']}")
    
    # çµæœã‚’JSONã§ä¿å­˜
    output_file = "data/kbo_source_investigation.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ“„ èª¿æŸ»çµæœã‚’ä¿å­˜: {output_file}")
    
    print(f"\nğŸ“‹ èª¿æŸ»ã‚µãƒãƒªãƒ¼:")
    for name, data in results.items():
        if data:
            print(f"  {name}: {data['total_links']}ãƒªãƒ³ã‚¯, é¸æ‰‹é–¢é€£{len(data.get('player_links', []))}å€‹")
        else:
            print(f"  {name}: ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯")

if __name__ == "__main__":
    main()