#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Yahooé‡çƒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æœ¬ç•ªé‹ç”¨ç‰ˆ
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒæ•´åˆæ€§ä¿®æ­£æ¸ˆã¿
"""

import requests
import sqlite3
from bs4 import BeautifulSoup
import time
import random
import os
import json
import logging
from datetime import datetime, timedelta
from contextlib import contextmanager
import re

# ===== è¨­å®š =====
BASE_URL = "https://baseball.yahoo.co.jp/npb"
DATA_DIR = "data/yahoo_continuous"
DB_PATH = os.path.join(DATA_DIR, "yahoo_games.db")
LOG_PATH = os.path.join(DATA_DIR, "scraper.log")
STATE_FILE = os.path.join(DATA_DIR, "scraper_state.json")

# ã‚¿ã‚¤ãƒŸãƒ³ã‚°è¨­å®š
GAME_PROCESSING_TIME = 2700  # 45åˆ† = 2700ç§’
PITCH_DELAY_MIN = 20   # æœ€å°20ç§’é–“éš”
PITCH_DELAY_MAX = 30   # æœ€å¤§30ç§’é–“éš”
REQUEST_TIMEOUT = 30

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
os.makedirs(DATA_DIR, exist_ok=True)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_PATH, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class YahooScraperProductionReady:
    def __init__(self):
        self.session = requests.Session()
        self.setup_session()
        self.db_path = DB_PATH
        self.state_file = STATE_FILE
        self.init_database()
        self.load_state()
        
    def setup_session(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–"""
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def get_random_headers(self):
        """ãƒ©ãƒ³ãƒ€ãƒ ãƒ˜ãƒƒãƒ€ãƒ¼ç”Ÿæˆ"""
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
        ]
        return {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en;q=0.5'
        }
    
    def init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ– - æ—¢å­˜ã‚¹ã‚­ãƒ¼ãƒã«åˆã‚ã›ã‚‹"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS games (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    game_id TEXT UNIQUE,
                    date TEXT,
                    home_team TEXT,
                    away_team TEXT,
                    status TEXT,
                    processed INTEGER DEFAULT 0,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # æ—¢å­˜ã®ã‚¹ã‚­ãƒ¼ãƒã«åˆã‚ã›ã‚‹: count_data -> count, zone -> zone
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS pitch_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    game_id TEXT,
                    index_code TEXT,
                    pitcher_name TEXT,
                    batter_name TEXT,
                    pitch_sequence INTEGER,
                    pitch_type TEXT,
                    velocity TEXT,
                    result TEXT,
                    count TEXT,
                    zone TEXT,
                    runners TEXT,
                    scraped_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(game_id, index_code, pitch_sequence)
                )
            ''')
            conn.commit()
    
    @contextmanager
    def get_connection(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç®¡ç†"""
        conn = sqlite3.connect(self.db_path)
        try:
            yield conn
        finally:
            conn.close()
    
    def load_state(self):
        """çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            with open(self.state_file, 'r', encoding='utf-8') as f:
                self.state = json.load(f)
        except FileNotFoundError:
            self.state = {
                'last_processed_date': (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d'),
                'total_games_processed': 0,
                'total_pitches_collected': 0,
                'session_start_time': datetime.now().isoformat()
            }
            self.save_state()
    
    def save_state(self):
        """çŠ¶æ…‹ä¿å­˜"""
        self.state['last_updated'] = datetime.now().isoformat()
        with open(self.state_file, 'w', encoding='utf-8') as f:
            json.dump(self.state, f, ensure_ascii=False, indent=2)
    
    def extract_pitch_data_production(self, game_id, index_code):
        """æœ¬ç•ªç”¨ä¸€çƒé€Ÿå ±ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        try:
            url = f"{BASE_URL}/game/{game_id}/score?index={index_code}"
            headers = self.get_random_headers()
            
            response = self.session.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, 'html.parser')
            
            if response.status_code != 200:
                logger.debug(f"HTTP {response.status_code}: {url}")
                return []
            
            # æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³: bb-splitsTableã‹ã‚‰æ­£ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®š
            all_tables = soup.select("table.bb-splitsTable")
            
            target_table = None
            for table in all_tables:
                headers_in_table = [th.text.strip() for th in table.select("thead th")]
                
                # å¿…è¦ãªãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
                if len(headers_in_table) >= 4:
                    required_headers = ['æŠ•çƒæ•°', 'çƒç¨®', 'çƒé€Ÿ', 'çµæœ']
                    if all(header in headers_in_table for header in required_headers):
                        target_table = table
                        break
            
            if not target_table:
                return []
            
            # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’å‡¦ç†
            pitches = []
            rows = target_table.select("tbody tr")
            
            for row in rows:
                # æŠ•çƒã‚¢ã‚¤ã‚³ãƒ³ãŒã‚ã‚‹è¡Œã®ã¿å‡¦ç†
                if not row.select_one("td span.bb-icon__ballCircle"):
                    continue
                
                cells = [td.text.strip() for td in row.select("td")]
                
                # 5ã‚»ãƒ«å½¢å¼: ['1', '1', 'ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ', '144km/h', 'è¦‹é€ƒã—']
                if len(cells) >= 5:
                    pitch_record = {
                        'game_id': game_id,
                        'index_code': index_code,
                        'pitch_sequence': int(cells[0]) if cells[0].isdigit() else 1,  # æŠ•çƒæ•°_æ‰“å¸­å†…
                        'pitch_type': cells[2],      # çƒç¨®
                        'velocity': cells[3],        # çƒé€Ÿ
                        'result': cells[4],          # çµæœ
                        'count': f"{cells[0]}/{cells[1]}",  # æ‰“å¸­å†…/åˆè¨ˆ -> æ—¢å­˜ã‚¹ã‚­ãƒ¼ãƒã®countåˆ—
                        'zone': '',  # å¾Œã§åº§æ¨™ã‹ã‚‰è¨ˆç®—å¯èƒ½
                        'runners': '',
                        'pitcher_name': '',
                        'batter_name': ''
                    }
                    pitches.append(pitch_record)
            
            if pitches:
                logger.info(f"âœ… æŠ½å‡ºæˆåŠŸ: {len(pitches)}çƒ ({game_id}-{index_code})")
            
            return pitches
            
        except Exception as e:
            logger.error(f"ä¸€çƒé€Ÿå ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼ {game_id}-{index_code}: {e}")
            return []
    
    def get_game_indexes(self, game_id):
        """è©¦åˆã®æ‰“å¸­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§ã‚’å–å¾—"""
        try:
            url = f"{BASE_URL}/game/{game_id}/score"
            headers = self.get_random_headers()
            
            response = self.session.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, 'html.parser')
            
            if response.status_code != 200:
                return []
            
            # æ‰“å¸­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŠ½å‡º
            indexes = set()
            for link in soup.select('a[href*="score?index="]'):
                href = link.get('href', '')
                match = re.search(r'index=([^&]+)', href)
                if match:
                    indexes.add(match.group(1))
            
            logger.info(f"Found {len(indexes)} indexes for game {game_id}")
            return sorted(indexes)
            
        except Exception as e:
            logger.error(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼ {game_id}: {e}")
            return []
    
    def process_single_game(self, game_id):
        """1è©¦åˆã‚’å®Œå…¨å‡¦ç†"""
        logger.info(f"ğŸ¾ å‡¦ç†é–‹å§‹: {game_id}")
        
        indexes = self.get_game_indexes(game_id)
        if not indexes:
            logger.warning(f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœªå–å¾—: {game_id}")
            return 0, 0
        
        total_pitches = 0
        processed_indexes = 0
        
        for index_code in indexes:
            pitches = self.extract_pitch_data_production(game_id, index_code)
            
            if pitches:
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ - æ—¢å­˜ã‚¹ã‚­ãƒ¼ãƒã«åˆã‚ã›ã‚‹
                with self.get_connection() as conn:
                    cursor = conn.cursor()
                    saved_count = 0
                    for pitch in pitches:
                        try:
                            cursor.execute('''
                                INSERT OR IGNORE INTO pitch_data 
                                (game_id, index_code, pitcher_name, batter_name, pitch_sequence,
                                 pitch_type, velocity, result, count, zone, runners)
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            ''', (
                                pitch['game_id'], pitch['index_code'], pitch['pitcher_name'],
                                pitch['batter_name'], pitch['pitch_sequence'], pitch['pitch_type'],
                                pitch['velocity'], pitch['result'], pitch['count'], pitch['zone'], pitch['runners']
                            ))
                            saved_count += 1
                        except sqlite3.Error as e:
                            logger.debug(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
                    conn.commit()
                
                total_pitches += len(pitches)
                processed_indexes += 1
                logger.info(f"  ğŸ“Š {index_code}: {len(pitches)}çƒä¿å­˜")
            
            # é…å»¶ï¼ˆä¸å¯§ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼‰
            delay = random.uniform(PITCH_DELAY_MIN, PITCH_DELAY_MAX)
            time.sleep(delay)
        
        logger.info(f"âœ… å®Œäº†: {game_id} - {processed_indexes}æ‰“å¸­, {total_pitches}çƒ")
        return processed_indexes, total_pitches
    
    def run_continuous_scraping(self):
        """é€£ç¶šã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ"""
        logger.info("ğŸš€ Yahooé‡çƒé€£ç¶šã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ (æœ¬ç•ªç‰ˆ)")
        
        # ãƒ†ã‚¹ãƒˆ: æˆåŠŸä¾‹ã§å‹•ä½œç¢ºèª
        test_game_id = "2021030362"
        logger.info(f"ğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: {test_game_id}")
        test_indexes, test_pitches = self.process_single_game(test_game_id)
        
        if test_pitches > 0:
            logger.info(f"âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸ! {test_pitches}çƒåé›†")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç¢ºèª
            with self.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM pitch_data")
                db_count = cursor.fetchone()[0]
                logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ç·çƒæ•°: {db_count}")
        else:
            logger.warning("âŒ ãƒ†ã‚¹ãƒˆå¤±æ•— - ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ç¢ºèªè¦")
            return
        
        # æœ¬æ ¼ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹
        target_games = [
            "2021030362", "2021030302", "2021030322", "2021030342", 
            "2021030401", "2021030402", "2021030421", "2021030441",
            "2021030501", "2021030521", "2021030541", "2021030561"
        ]
        
        while True:
            try:
                for game_id in target_games:
                    # æ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
                    with self.get_connection() as conn:
                        cursor = conn.cursor()
                        cursor.execute('SELECT COUNT(*) FROM pitch_data WHERE game_id = ?', (game_id,))
                        existing_count = cursor.fetchone()[0]
                    
                    if existing_count > 0:
                        logger.info(f"â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: {game_id} (æ—¢ã«{existing_count}çƒåé›†æ¸ˆã¿)")
                        continue
                    
                    # è©¦åˆã‚’å‡¦ç†
                    start_time = time.time()
                    indexes_processed, pitches_collected = self.process_single_game(game_id)
                    processing_time = time.time() - start_time
                    
                    # çµ±è¨ˆæ›´æ–°
                    if pitches_collected > 0:
                        self.state['total_games_processed'] += 1
                        self.state['total_pitches_collected'] += pitches_collected
                        self.save_state()
                        
                        logger.info(f"ğŸ“ˆ ç´¯è¨ˆ: {self.state['total_games_processed']}è©¦åˆ, {self.state['total_pitches_collected']}çƒ")
                    
                    # 45åˆ†é–“éš”ã®èª¿æ•´
                    remaining_time = GAME_PROCESSING_TIME - processing_time
                    if remaining_time > 0:
                        logger.info(f"â³ æ¬¡ã®è©¦åˆã¾ã§ {remaining_time:.0f}ç§’å¾…æ©Ÿ")
                        time.sleep(remaining_time)
                
                # å…¨è©¦åˆå‡¦ç†å¾Œã¯æ–°ã—ã„è©¦åˆã‚’æ¢ã™
                logger.info("ğŸ”„ å‡¦ç†å®Œäº† - æ–°ã—ã„è©¦åˆã‚’æ¤œç´¢ä¸­...")
                time.sleep(1800)  # 30åˆ†å¾…æ©Ÿ
                
            except KeyboardInterrupt:
                logger.info("ğŸ›‘ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°åœæ­¢")
                break
            except Exception as e:
                logger.error(f"âŒ ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(300)  # 5åˆ†å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    scraper = YahooScraperProductionReady()
    scraper.run_continuous_scraping()

if __name__ == "__main__":
    main()