import glob
import os
import time
import requests
import pandas as pd
from bs4 import BeautifulSoup
import random
import hashlib
import json
from urllib.robotparser import RobotFileParser
from datetime import datetime, timedelta

# â€” è¨­å®š â€”
GAME_INFO_DIR = "fetch/data/game_info"  # æ—¥åˆ¥ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«CSVãƒ•ã‚©ãƒ«ãƒ€
OUTPUT_DIR    = "data/valid_indexes"    # æ‰“å¸­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡ºåŠ›å…ˆ
CACHE_DIR     = "data/cache/indexes"    # HTMLã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
ROBOTS_CACHE  = "data/cache/robots.txt" # robots.txtã‚­ãƒ£ãƒƒã‚·ãƒ¥

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¨­å®šï¼ˆå¼·åŒ–ï¼‰
MIN_DELAY = 5.0  # æœ€å°å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
MAX_DELAY = 10.0 # æœ€å¤§å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
CIRCUIT_BREAKER_THRESHOLD = 5  # é€£ç¶šå¤±æ•—ã§ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ä½œå‹•
CIRCUIT_BREAKER_COOLDOWN = 300  # ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³æ™‚é–“ï¼ˆç§’ï¼‰

# è­˜åˆ¥å­è¨­å®š
USER_AGENT = "Mozilla/5.0 (compatible; NPB-DataCollector/1.0; +contact@example.com)"
FROM_HEADER = "contact@example.com"

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(CACHE_DIR, exist_ok=True)
os.makedirs(os.path.dirname(ROBOTS_CACHE), exist_ok=True)

# Circuit BreakerçŠ¶æ…‹
circuit_breaker_state = {
    'failures': 0,
    'last_failure': None,
    'is_open': False
}

def check_robots_txt():
    """
    robots.txtã‚’ç¢ºèªã—ã¦ã€ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
    """
    try:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰robots.txtã‚’èª­ã¿è¾¼ã¿ï¼ˆ24æ™‚é–“æœ‰åŠ¹ï¼‰
        if os.path.exists(ROBOTS_CACHE):
            cache_age = time.time() - os.path.getmtime(ROBOTS_CACHE)
            if cache_age < 86400:  # 24æ™‚é–“
                with open(ROBOTS_CACHE, 'r', encoding='utf-8') as f:
                    robots_content = f.read()
            else:
                robots_content = None
        else:
            robots_content = None
        
        if not robots_content:
            # robots.txtã‚’å–å¾—
            response = requests.get('https://baseball.yahoo.co.jp/robots.txt', timeout=10)
            robots_content = response.text
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            with open(ROBOTS_CACHE, 'w', encoding='utf-8') as f:
                f.write(robots_content)
        
        # robots.txtã‚’ãƒ‘ãƒ¼ã‚¹
        rp = RobotFileParser()
        rp.set_url('https://baseball.yahoo.co.jp/robots.txt')
        rp.read()
        
        # /npb/game/ ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒè¨±å¯ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        can_fetch = rp.can_fetch(USER_AGENT, 'https://baseball.yahoo.co.jp/npb/game/')
        
        if not can_fetch:
            print("âš ï¸ robots.txt ã«ã‚ˆã‚Š /npb/game/ ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒç¦æ­¢ã•ã‚Œã¦ã„ã¾ã™")
            print("NPBå…¬å¼ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™")
            return False
        
        return True
        
    except Exception as e:
        print(f"robots.txtç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        print("robots.txtãŒç¢ºèªã§ãã¾ã›ã‚“ãŒã€å‡¦ç†ã‚’ç¶™ç¶šã—ã¾ã™")
        return True

def circuit_breaker_check():
    """
    Circuit Breakerã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
    """
    if circuit_breaker_state['is_open']:
        if circuit_breaker_state['last_failure']:
            elapsed = time.time() - circuit_breaker_state['last_failure']
            if elapsed > CIRCUIT_BREAKER_COOLDOWN:
                # ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³å®Œäº†ã€å›å¾©ã‚’è©¦è¡Œ
                circuit_breaker_state['is_open'] = False
                circuit_breaker_state['failures'] = 0
                print(f"ğŸ”„ Circuit breaker recovered after {elapsed:.0f}s cooldown")
                return True
            else:
                remaining = CIRCUIT_BREAKER_COOLDOWN - elapsed
                print(f"âš ï¸ Circuit breaker open - {remaining:.0f}s remaining")
                return False
    
    return True

def record_success():
    """
    æˆåŠŸã‚’è¨˜éŒ²ã—ã¦Circuit Breakerã‚’ãƒªã‚»ãƒƒãƒˆ
    """
    circuit_breaker_state['failures'] = 0
    circuit_breaker_state['is_open'] = False

def record_failure():
    """
    å¤±æ•—ã‚’è¨˜éŒ²ã—ã¦Circuit Breakerã‚’æ›´æ–°
    """
    circuit_breaker_state['failures'] += 1
    circuit_breaker_state['last_failure'] = time.time()
    
    if circuit_breaker_state['failures'] >= CIRCUIT_BREAKER_THRESHOLD:
        circuit_breaker_state['is_open'] = True
        print(f"ğŸš¨ Circuit breaker opened after {circuit_breaker_state['failures']} failures")

def get_cache_path(url):
    """
    URLã‹ã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆ
    """
    url_hash = hashlib.md5(url.encode()).hexdigest()
    return os.path.join(CACHE_DIR, f"{url_hash}.json")

def get_cached_response(url, max_age_hours=24):
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—ï¼ˆ24æ™‚é–“æœ‰åŠ¹ï¼‰
    """
    cache_path = get_cache_path(url)
    
    if os.path.exists(cache_path):
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            cache_time = datetime.fromisoformat(cache_data['timestamp'])
            if datetime.now() - cache_time < timedelta(hours=max_age_hours):
                return cache_data['content']
        except Exception as e:
            print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    return None

def save_to_cache(url, content):
    """
    ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    """
    cache_path = get_cache_path(url)
    
    try:
        cache_data = {
            'url': url,
            'content': content,
            'timestamp': datetime.now().isoformat()
        }
        
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False)
    except Exception as e:
        print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

def safe_request(url, session):
    """
    ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    """
    # Circuit Breaker ãƒã‚§ãƒƒã‚¯
    if not circuit_breaker_check():
        return None
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
    cached_content = get_cached_response(url)
    if cached_content:
        record_success()
        return cached_content
    
    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
    sleep_time = random.uniform(MIN_DELAY, MAX_DELAY)
    print(f"  Waiting {sleep_time:.1f}s before request...")
    time.sleep(sleep_time)
    
    try:
        headers = {
            'User-Agent': USER_AGENT,
            'From': FROM_HEADER,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        response = session.get(url, headers=headers, timeout=15)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ç¢ºèª
        if response.status_code == 429:
            # Too Many Requests - Retry-Afterãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç¢ºèª
            retry_after = response.headers.get('Retry-After')
            if retry_after:
                wait_time = int(retry_after)
                print(f"âš ï¸ Rate limited (429) - waiting {wait_time}s as requested")
                time.sleep(wait_time)
            else:
                # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                wait_time = min(300, 2 ** circuit_breaker_state['failures'])
                print(f"âš ï¸ Rate limited (429) - exponential backoff {wait_time}s")
                time.sleep(wait_time)
            
            record_failure()
            return None
        
        elif response.status_code == 503:
            # Service Unavailable
            print(f"âš ï¸ Service unavailable (503) - backing off")
            record_failure()
            return None
        
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        save_to_cache(url, response.text)
        record_success()
        
        return response.text
        
    except requests.exceptions.RequestException as e:
        print(f"Request error for {url}: {e}")
        record_failure()
        return None

def extract_index_links_from_score_page(game_id: str, session: requests.Session) -> list[str]:
    """
    ãƒªãƒ³ã‚¯ã‚¹ã‚­ãƒ£ãƒ³æ–¹å¼: /score ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨ã¦ã®æœ‰åŠ¹ãªindexãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
    """
    score_url = f"https://baseball.yahoo.co.jp/npb/game/{game_id}/score"
    
    print(f"ğŸ“‹ Scanning index links from score page: {game_id}")
    
    html = safe_request(score_url, session)
    if not html:
        print(f"  âŒ Failed to fetch score page for {game_id}")
        return []
    
    soup = BeautifulSoup(html, "html.parser")
    
    # æ‰“å¸­é¸æŠã®ã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹ã‚„ãƒªãƒ³ã‚¯ã‹ã‚‰indexã‚’æŠ½å‡º
    valid_indexes = set()
    
    # æ–¹æ³•1: selectã‚¿ã‚°ã®optionã‹ã‚‰indexå€¤ã‚’æŠ½å‡º
    select_tags = soup.find_all('select')
    for select in select_tags:
        options = select.find_all('option')
        for option in options:
            if 'value' in option.attrs:
                value = option['value']
                # indexã®å½¢å¼ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆä¾‹: 010100, 010200 ãªã©ï¼‰
                if re.match(r'\d{6}', value):
                    valid_indexes.add(value)
    
    # æ–¹æ³•2: ?index= ã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‹ã‚‰indexå€¤ã‚’æŠ½å‡º
    links = soup.find_all('a', href=True)
    for link in links:
        href = link['href']
        match = re.search(r'[?&]index=(\d{6})', href)
        if match:
            valid_indexes.add(match.group(1))
    
    # æ–¹æ³•3: JavaScriptã§å‹•çš„ã«ç”Ÿæˆã•ã‚Œã‚‹å ´åˆã®å¯¾å¿œ
    # data-index å±æ€§ã‚„ã‚¯ãƒ©ã‚¹åã‹ã‚‰indexå€¤ã‚’æ¨æ¸¬
    data_elements = soup.find_all(attrs={'data-index': True})
    for elem in data_elements:
        index_value = elem['data-index']
        if re.match(r'\d{6}', index_value):
            valid_indexes.add(index_value)
    
    sorted_indexes = sorted(list(valid_indexes))
    
    if sorted_indexes:
        print(f"  âœ… Found {len(sorted_indexes)} index links via scanning")
        return sorted_indexes
    else:
        print(f"  âš ï¸ No index links found via scanning - falling back to adjacent navigation")
        return extract_via_adjacent_navigation(game_id, session)

def extract_via_adjacent_navigation(game_id: str, session: requests.Session) -> list[str]:
    """
    éš£æ¥é·ç§»æ–¹å¼: æœ€åˆã®indexã‹ã‚‰ã€Œå‰/æ¬¡ã€ãƒªãƒ³ã‚¯ã‚’è¾¿ã£ã¦å…¨æ‰“å¸­ã‚’ã‚«ãƒãƒ¼
    """
    print(f"ğŸ”„ Using adjacent navigation for {game_id}")
    
    # é–‹å§‹ç‚¹ã‚’è¦‹ã¤ã‘ã‚‹ï¼ˆé€šå¸¸ã¯æœ€åˆã®æ‰“å¸­: 010100ï¼‰
    start_indexes = ['010100', '010101', '010200', '020100']
    
    valid_indexes = set()
    visited = set()
    
    for start_idx in start_indexes:
        if start_idx in visited:
            continue
            
        current_idx = start_idx
        base_url = f"https://baseball.yahoo.co.jp/npb/game/{game_id}/score?index="
        
        while current_idx and current_idx not in visited:
            visited.add(current_idx)
            url = base_url + current_idx
            
            html = safe_request(url, session)
            if not html:
                break
            
            soup = BeautifulSoup(html, "html.parser")
            
            # splitsTableãŒã‚ã‚Œã°æœ‰åŠ¹ãªindex
            if soup.select_one("table.bb-splitsTable"):
                valid_indexes.add(current_idx)
                print(f"  âœ… Valid index found: {current_idx}")
            
            # æ¬¡ã®indexã‚’æ¢ã™
            next_idx = None
            
            # ã€Œæ¬¡ã¸ã€ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            next_links = soup.find_all('a', href=True)
            for link in next_links:
                href = link['href']
                text = link.get_text(strip=True)
                
                if 'æ¬¡' in text or 'next' in text.lower():
                    match = re.search(r'[?&]index=(\d{6})', href)
                    if match:
                        next_idx = match.group(1)
                        break
            
            # ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯é †åºæ¨æ¸¬
            if not next_idx:
                next_idx = guess_next_index(current_idx)
            
            current_idx = next_idx
            
            # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
            if len(visited) > 500:  # ç•°å¸¸ã«å¤šã„å ´åˆã¯åœæ­¢
                print(f"  âš ï¸ Too many indexes visited, stopping navigation")
                break
    
    return sorted(list(valid_indexes))

def guess_next_index(current_idx: str) -> str:
    """
    ç¾åœ¨ã®indexã‹ã‚‰æ¬¡ã®indexã‚’æ¨æ¸¬
    """
    if len(current_idx) != 6:
        return None
    
    try:
        inning = int(current_idx[:2])
        side = int(current_idx[2])
        batter = int(current_idx[3:5])
        pitch = int(current_idx[5])
        
        # åŒã˜æ‰“å¸­ã®æ¬¡ã®çƒ
        if pitch == 0:
            return f"{inning:02d}{side}{batter:02d}01"
        
        # æ¬¡ã®æ‰“è€…
        if batter < 9:
            return f"{inning:02d}{side}{batter+1:02d}00"
        
        # æ¬¡ã®ã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆè¡¨â†’è£ã€è£â†’æ¬¡ã‚¤ãƒ‹ãƒ³ã‚°è¡¨ï¼‰
        if side == 1:
            return f"{inning:02d}2{1:02d}00"
        else:
            return f"{inning+1:02d}1{1:02d}00"
            
    except ValueError:
        return None

def extract_valid_indexes(game_id: str) -> list[str]:
    """
    æ”¹å–„ç‰ˆ: ãƒªãƒ³ã‚¯ã‚¹ã‚­ãƒ£ãƒ³ + éš£æ¥é·ç§»ã§indexã‚’æŠ½å‡º
    """
    with requests.Session() as session:
        # ã¾ãšãƒªãƒ³ã‚¯ã‚¹ã‚­ãƒ£ãƒ³ã‚’è©¦è¡Œ
        indexes = extract_index_links_from_score_page(game_id, session)
        
        # ãƒªãƒ³ã‚¯ã‚¹ã‚­ãƒ£ãƒ³ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯éš£æ¥é·ç§»
        if not indexes:
            indexes = extract_via_adjacent_navigation(game_id, session)
        
        return indexes

def main():
    print("â–¶ Step 2 (Improved): Extracting valid indexes via link scanning")
    
    # robots.txtç¢ºèª
    if not check_robots_txt():
        print("âŒ robots.txtã«ã‚ˆã‚Šã‚¢ã‚¯ã‚»ã‚¹ãŒåˆ¶é™ã•ã‚Œã¦ã„ã¾ã™")
        print("NPBå…¬å¼ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’å„ªå…ˆã—ã¦ä½¿ç”¨ã—ã¦ãã ã•ã„")
        return
    
    # æ—¥åˆ¥CSVã‚’èª­ã¿è¾¼ã‚“ã§çµåˆ
    csvs = sorted(glob.glob(os.path.join(GAME_INFO_DIR, "*.csv")))
    if not csvs:
        raise FileNotFoundError(f"No CSV files found in {GAME_INFO_DIR}. Please run step_1_schedule_scraper.py first.")

    dataframes = []
    for p in csvs:
        try:
            if os.path.getsize(p) == 0:
                continue
            df_temp = pd.read_csv(p, dtype=str)
            if not df_temp.empty:
                dataframes.append(df_temp)
        except Exception as e:
            print(f"Error reading {p}: {e}")

    if not dataframes:
        print("âš  No valid game info data found")
        return

    df = pd.concat(dataframes, ignore_index=True)

    # è©¦åˆçµ‚äº†ã—ã¦ã„ã‚‹æœ€æ–°ã®è©¦åˆã‚’ãƒ†ã‚¹ãƒˆå¯¾è±¡ã«ã™ã‚‹
    df['è©¦åˆæ—¥_dt'] = pd.to_datetime(df['è©¦åˆæ—¥'])
    df_sorted = df.sort_values(by='è©¦åˆæ—¥_dt', ascending=False)
    
    test_row = None
    for index, row in df_sorted.iterrows():
        if row["è©¦åˆçŠ¶æ…‹"] == "è©¦åˆçµ‚äº†":
            test_row = row
            break
            
    if test_row is None:
        print("âš  å‡¦ç†ã§ãã‚‹ã€è©¦åˆçµ‚äº†ã€ã®è©¦åˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return

    test_gid = test_row["game_id"]
    print(f"ğŸ¯ Testing improved extraction for game_id={test_gid}")
    
    start_time = time.time()
    indexes = extract_valid_indexes(test_gid)
    elapsed = time.time() - start_time
    
    if indexes:
        out = os.path.join(OUTPUT_DIR, f"valid_indexes_{test_gid}.csv")
        pd.DataFrame(indexes, columns=["index"]).to_csv(out, index=False, encoding="utf-8-sig")
        print(f"âœ… Extracted {len(indexes)} indexes in {elapsed:.1f}s â†’ {out}")
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‰Šæ¸›åŠ¹æœã‚’è¡¨ç¤º
        estimated_old_requests = 9 * 9 * 2 * 2 * get_max_inning(test_gid)  # æ—§æ–¹å¼ã®æ¨å®šãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
        actual_requests = circuit_breaker_state.get('total_requests', len(indexes) + 1)
        reduction = max(0, estimated_old_requests - actual_requests)
        
        print(f"ğŸ“Š Request reduction: {reduction} fewer requests vs brute-force")
        print(f"ğŸ”„ Circuit breaker state: {circuit_breaker_state['failures']} failures")
    else:
        print(f"âš  No indexes found for game_id={test_gid}")

def get_max_inning(game_id: str) -> int:
    """
    ã‚¹ã‚³ã‚¢è¡¨ã‹ã‚‰æœ€å¤§ã‚¤ãƒ‹ãƒ³ã‚°æ•°ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
    """
    url = f"https://baseball.yahoo.co.jp/npb/game/{game_id}/score"
    
    try:
        with requests.Session() as session:
            html = safe_request(url, session)
            if not html:
                return 9
                
            soup = BeautifulSoup(html, "html.parser")
            ths = soup.select("table.bb-gameScoreTable thead th")
            if ths and len(ths) >= 4:
                return len(ths) - 3
    except Exception as e:
        print(f"Error getting max inning for {game_id}: {e}")
    
    return 9

if __name__ == "__main__":
    main()