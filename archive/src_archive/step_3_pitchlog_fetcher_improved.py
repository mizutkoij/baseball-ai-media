# step_3_pitchlog_fetcher_improved.py (ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¼·åŒ–ãƒ»å¤œé–“ãƒãƒƒã‚¯ãƒ•ã‚£ãƒ«å¯¾å¿œç‰ˆ)

import glob, os, re, time, pandas as pd
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
from datetime import datetime, time as dt_time
import random
import json

VALID_IDX_DIR   = "data/valid_indexes"
OUTPUT_DIR      = "data/pitch_logs_improved"
DEBUG_HTML_DIR  = "data/debug_html"
CACHE_DIR       = "data/cache/pitchlogs"

# æ™‚é–“å¸¯åˆ¶å¾¡
NIGHT_BACKFILL_START = dt_time(22, 0)  # 22:00
NIGHT_BACKFILL_END = dt_time(6, 0)     # 06:00
LIVE_HOURS = [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¨­å®š
MIN_DELAY = 3.0
MAX_DELAY = 8.0
CONCURRENT_LIMIT = 1  # åŒæ™‚å®Ÿè¡Œæ•°åˆ¶é™

# Circuit breaker
CIRCUIT_BREAKER_THRESHOLD = 3
CIRCUIT_BREAKER_COOLDOWN = 180

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(DEBUG_HTML_DIR, exist_ok=True)
os.makedirs(CACHE_DIR, exist_ok=True)

circuit_breaker_state = {
    'failures': 0,
    'last_failure': None,
    'is_open': False
}

def is_night_backfill_time():
    """
    å¤œé–“ãƒãƒƒã‚¯ãƒ•ã‚£ãƒ«æ™‚é–“å¸¯ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
    """
    current_time = datetime.now().time()
    
    # 22:00-23:59 ã¾ãŸã¯ 00:00-06:00
    return (current_time >= NIGHT_BACKFILL_START or 
            current_time <= NIGHT_BACKFILL_END)

def should_process_game(game_id, is_backfill=False):
    """
    ã‚²ãƒ¼ãƒ å‡¦ç†å¯å¦ã®åˆ¤å®š
    """
    current_hour = datetime.now().hour
    
    if is_backfill:
        # ãƒãƒƒã‚¯ãƒ•ã‚£ãƒ«ã¯å¤œé–“ã®ã¿
        return is_night_backfill_time()
    else:
        # ãƒ©ã‚¤ãƒ–å‡¦ç†ã¯æ—¥ä¸­ã®ã¿
        return current_hour in LIVE_HOURS

def circuit_breaker_check():
    """
    Circuit Breakerã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
    """
    if circuit_breaker_state['is_open']:
        if circuit_breaker_state['last_failure']:
            elapsed = time.time() - circuit_breaker_state['last_failure']
            if elapsed > CIRCUIT_BREAKER_COOLDOWN:
                circuit_breaker_state['is_open'] = False
                circuit_breaker_state['failures'] = 0
                print(f"ğŸ”„ Circuit breaker recovered after {elapsed:.0f}s")
                return True
            else:
                print(f"âš ï¸ Circuit breaker open - {elapsed:.0f}s elapsed")
                return False
    return True

def record_failure():
    """
    å¤±æ•—ã‚’è¨˜éŒ²
    """
    circuit_breaker_state['failures'] += 1
    circuit_breaker_state['last_failure'] = time.time()
    
    if circuit_breaker_state['failures'] >= CIRCUIT_BREAKER_THRESHOLD:
        circuit_breaker_state['is_open'] = True
        print(f"ğŸš¨ Circuit breaker opened after {circuit_breaker_state['failures']} failures")

def record_success():
    """
    æˆåŠŸã‚’è¨˜éŒ²
    """
    circuit_breaker_state['failures'] = 0

def get_cache_key(game_id, idx):
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
    """
    return f"{game_id}_{idx}"

def load_from_cache(cache_key):
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆ30æ—¥æœ‰åŠ¹ï¼‰
    """
    cache_file = os.path.join(CACHE_DIR, f"{cache_key}.json")
    
    if os.path.exists(cache_file):
        try:
            cache_age = time.time() - os.path.getmtime(cache_file)
            if cache_age < 30 * 24 * 3600:  # 30æ—¥
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"Cache read error for {cache_key}: {e}")
    
    return None

def save_to_cache(cache_key, data):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    """
    cache_file = os.path.join(CACHE_DIR, f"{cache_key}.json")
    
    try:
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, default=str)
    except Exception as e:
        print(f"Cache save error for {cache_key}: {e}")

def adaptive_delay():
    """
    é©å¿œçš„ãªé…å»¶ï¼ˆå¤±æ•—å›æ•°ã«å¿œã˜ã¦å¢—åŠ ï¼‰
    """
    base_delay = random.uniform(MIN_DELAY, MAX_DELAY)
    failure_multiplier = 1 + (circuit_breaker_state['failures'] * 0.5)
    actual_delay = min(base_delay * failure_multiplier, 30.0)  # æœ€å¤§30ç§’
    
    print(f"  Adaptive delay: {actual_delay:.1f}s (failures: {circuit_breaker_state['failures']})")
    time.sleep(actual_delay)

def classify_zone(top, left):
    """
    ãƒ”ã‚¯ã‚»ãƒ«åº§æ¨™ã‹ã‚‰ã‚¾ãƒ¼ãƒ³ã‚’åˆ†é¡ã™ã‚‹
    """
    if top < 60:      v = "é«˜ã‚"
    elif top < 120:   v = "ä¸­"
    else:             v = "ä½ã‚"
    if left < 60:     h = "å¤–è§’"
    elif left < 120:  h = "çœŸã‚“ä¸­"
    else:             h = "å†…è§’"
    return f"{h}{v}"

def fetch_pitches_for_index_with_cache(page, game_id, idx):
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œç‰ˆã®æŠ•çƒãƒ‡ãƒ¼ã‚¿å–å¾—
    """
    cache_key = get_cache_key(game_id, idx)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç¢ºèª
    cached_data = load_from_cache(cache_key)
    if cached_data is not None:
        print(f"  ğŸ“„ Cache hit for {idx}")
        record_success()
        return cached_data
    
    # Circuit breakerç¢ºèª
    if not circuit_breaker_check():
        return []
    
    # é©å¿œçš„é…å»¶
    adaptive_delay()
    
    url = f"https://baseball.yahoo.co.jp/npb/game/{game_id}/score?index={idx}"
    
    try:
        page.goto(url, timeout=30000)
        
        # è©¦åˆã”ã¨ã®HTMLä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        game_html_dir = os.path.join(DEBUG_HTML_DIR, game_id)
        os.makedirs(game_html_dir, exist_ok=True)

        try:
            page.wait_for_selector("div#async-fieldBody", timeout=20000)
        except Exception as e:
            print(f"   Error: div#async-fieldBody not found for {url}")
            record_failure()
            
            # ã‚¨ãƒ©ãƒ¼HTMLä¿å­˜
            error_html_path = os.path.join(game_html_dir, f"error_{idx}_no_main_content.html")
            with open(error_html_path, "w", encoding="utf-8") as f:
                f.write(page.content())
            return []

        html = page.content()
        
        # æˆåŠŸHTMLä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        if random.random() < 0.1:  # 10%ã®ç¢ºç‡ã§ä¿å­˜
            success_html_path = os.path.join(game_html_dir, f"success_{idx}_sample.html")
            with open(success_html_path, "w", encoding="utf-8") as f:
                f.write(html)
                
        soup = BeautifulSoup(html, "html.parser")

        # ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå…ƒã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ï¼‰
        batter_name = None
        batter_hand = None
        pitcher_name = None
        pitcher_hand = None
        runner_on_1b = False
        runner_on_2b = False
        runner_on_3b = False

        batter_card = soup.select_one("div#batter table.ct")
        if batter_card:
            batter_name_tag = batter_card.select_one("td.nm a")
            if batter_name_tag:
                batter_name = batter_name_tag.get_text(strip=True)
            batter_hand_tag = batter_card.select_one("td.dominantHand")
            if batter_hand_tag:
                batter_hand = batter_hand_tag.get_text(strip=True)

        pitcher_card = soup.select_one("div#pit div#pitcherR table.ct")
        if pitcher_card:
            pitcher_name_tag = pitcher_card.select_one("td.nm a")
            if pitcher_name_tag:
                pitcher_name = pitcher_name_tag.get_text(strip=True)
            pitcher_hand_tag = pitcher_card.select_one("td.dominantHand")
            if pitcher_hand_tag:
                pitcher_hand = pitcher_hand_tag.get_text(strip=True)

        base_div = soup.select_one("div#field div#base")
        if base_div and 'class' in base_div.attrs:
            base_class = base_div['class'][0] 
            runner_on_1b = '1' in base_class[1]
            runner_on_2b = '1' in base_class[2]
            runner_on_3b = '1' in base_class[3]

        pitch_details_section = soup.select_one("section.bb-splits__item:has(h3.bb-head02__title:-soup-contains('è©³ã—ã„æŠ•çƒå†…å®¹'))")
        
        table = None
        if pitch_details_section:
            table = pitch_details_section.select_one("table.bb-splitsTable:has(thead)")
        
        if not table:
            all_splits_tables = soup.select("table.bb-splitsTable")
            for tbl in all_splits_tables:
                ths_text = [th.text.strip() for th in tbl.select("thead th")]
                if "æŠ•çƒæ•°" in ths_text and "çƒç¨®" in ths_text and "çƒé€Ÿ" in ths_text and "çµæœ" in ths_text:
                    table = tbl
                    break

        if not table:
            print(f"   Warning: No valid pitch log table found for {url}")
            record_failure()
            return []

        head_row = table.select_one("thead tr")
        if not head_row:
            print(f"   Warning: No header row found for {url}")
            return []

        headers = []
        for th in head_row.select("th.bb-splitsTable__head"):
            th_text = th.text.strip()
            colspan = int(th.get('colspan', 1))
            
            if colspan > 1:
                if th_text == "æŠ•çƒæ•°":
                    headers.append("æŠ•çƒæ•°_æ‰“å¸­å†…")
                    headers.append("æŠ•çƒæ•°_åˆè¨ˆ")
                else:
                    for i in range(colspan):
                        headers.append(f"{th_text}_{i+1}")
            else:
                headers.append(th_text)

        if len(headers) != 5 or not all(keyword in headers for keyword in ["çƒç¨®", "çƒé€Ÿ", "çµæœ"]):
            print(f"   Warning: Invalid headers for {url}. Headers: {headers}")
            return []

        rows = table.select("tbody tr")
        
        pitch_logs = []
        for i, tr in enumerate(rows, start=1):
            if not tr.select_one("td span.bb-icon__ballCircle"):
                continue 

            cells = [td.text.strip() for td in tr.select("td")]
            
            if len(cells) != len(headers):
                print(f"   Warning: Cell/header mismatch for pitch {i} in {url}")
                continue
                
            rec = dict(zip(headers, cells))
            if 'çƒé€Ÿ' in rec and rec['çƒé€Ÿ'] == '-':
                rec['çƒé€Ÿ'] = None

            rec.update({
                "game_id": game_id,
                "index": idx,
                "pitch_no": str(rec["æŠ•çƒæ•°_æ‰“å¸­å†…"]),
                "æ‰“è€…å": batter_name,
                "æ‰“è€…åˆ©ãè…•": batter_hand,
                "æŠ•æ‰‹å": pitcher_name,
                "æŠ•æ‰‹åˆ©ãè…•": pitcher_hand,
                "èµ°è€…1å¡": runner_on_1b,
                "èµ°è€…2å¡": runner_on_2b,
                "èµ°è€…3å¡": runner_on_3b,
            })
            pitch_logs.append(rec)

        # é…çƒãƒãƒ£ãƒ¼ãƒˆåº§æ¨™ã®æŠ½å‡º
        locs = {}
        for span in soup.select("div#pitchesDetail div.bb-allocationChart span.bb-icon__ballCircle"):
            no_tag = span.select_one("span.bb-icon__number")
            if not no_tag: continue
            no = no_tag.text.strip()
            style = span.get("style")
            if not style: continue
            
            top_match  = re.search(r"top:(\d+\.?\d*)px", style)
            left_match = re.search(r"left:(\d+\.?\d*)px", style)

            if top_match and left_match:
                top  = float(top_match.group(1))
                left = float(left_match.group(1))
                locs[no] = (left, top)

        for span in soup.select("div.next div.bb-allocationChart span.bb-icon__ballCircle"):
            no_tag = span.select_one("span.bb-icon__number")
            if not no_tag: continue
            no = no_tag.text.strip()
            style = span.get("style")
            if not style: continue
            
            top_match  = re.search(r"top:(\d+\.?\d*)px", style)
            left_match = re.search(r"left:(\d+\.?\d*)px", style)

            if top_match and left_match:
                top  = float(top_match.group(1))
                left = float(left_match.group(1))
                locs[no] = (left, top)

        # åº§æ¨™ã¨ã‚¾ãƒ¼ãƒ³ã‚’è¿½åŠ 
        for rec in pitch_logs:
            no = rec["pitch_no"]
            if no in locs:
                x, y = locs[no]
                rec["x_px"], rec["y_px"] = x, y
                rec["zone"] = classify_zone(y, x)
            else:
                rec["x_px"] = rec["y_px"] = rec["zone"] = None

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        save_to_cache(cache_key, pitch_logs)
        record_success()
        
        return pitch_logs
        
    except Exception as e:
        print(f"   Error fetching {url}: {e}")
        record_failure()
        return []

def main():
    print("â–¶ Step 3 (Improved): Fetching pitch logs with enhanced rate limiting")
    
    # æ™‚é–“å¸¯ãƒã‚§ãƒƒã‚¯
    current_hour = datetime.now().hour
    is_backfill = len(glob.glob(f"{OUTPUT_DIR}/pitch_logs_*.csv")) > 0
    
    if not should_process_game("test", is_backfill):
        if is_backfill:
            print(f"â° Backfill processing is only allowed during night hours (22:00-06:00)")
        else:
            print(f"â° Live processing is only allowed during day hours (07:00-21:00)")
        print(f"Current time: {datetime.now().strftime('%H:%M')}")
        return
    
    csvs = sorted(glob.glob(f"{VALID_IDX_DIR}/valid_indexes_*.csv"))
    if not csvs:
        raise FileNotFoundError(f"No valid_indexes CSVs in {VALID_IDX_DIR}. Run step_2 first.")

    print(f"ğŸ• Processing at {datetime.now().strftime('%H:%M')} ({'backfill' if is_backfill else 'live'} mode)")

    with sync_playwright() as pw:
        browser = pw.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-gpu',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor'
            ]
        )
        
        context = browser.new_context(
            user_agent="Mozilla/5.0 (compatible; NPB-DataCollector/1.0; +contact@example.com)",
            viewport={'width': 1280, 'height': 720},
            extra_http_headers={
                'From': 'contact@example.com'
            }
        )
        
        page = context.new_page()
        processed_game_ids = set()

        for path in csvs:
            file_name = os.path.basename(path)
            game_id_match = re.search(r"valid_indexes_(\d+)\.csv$", file_name)
            if not game_id_match:
                print(f"   Skipping malformed CSV path: {path}")
                continue
            game_id = game_id_match.group(1)

            if game_id in processed_game_ids:
                print(f"   Skipping already processed game_id={game_id}")
                continue
            processed_game_ids.add(game_id)

            out_csv = f"{OUTPUT_DIR}/pitch_logs_{game_id}.csv"
            if os.path.exists(out_csv) and not is_backfill:
                print(f"   Output file exists for game_id={game_id}, skipping")
                continue

            if not should_process_game(game_id, is_backfill):
                print(f"   Skipping game_id={game_id} due to time restrictions")
                continue

            print(f"\nâ—† Processing game_id={game_id}")
            df_idx = pd.read_csv(path, dtype=str)
            all_logs = []
            
            cache_hits = 0
            for i, idx in enumerate(df_idx["index"]):
                if i % 5 == 0 or i == len(df_idx["index"]) - 1:
                    print(f"   Processing index {i+1}/{len(df_idx['index'])}: {idx}", end='\r')
                
                recs = fetch_pitches_for_index_with_cache(page, game_id, idx)
                if recs:
                    all_logs.extend(recs)
                    if load_from_cache(get_cache_key(game_id, idx)):
                        cache_hits += 1

            if all_logs:
                df_logs = pd.DataFrame(all_logs)
                df_logs.to_csv(out_csv, index=False, encoding="utf-8-sig")
                print(f"\n   âœ… Saved {len(df_logs)} records â†’ {out_csv}")
                print(f"   ğŸ“Š Cache hits: {cache_hits}/{len(df_idx)} ({cache_hits/len(df_idx)*100:.1f}%)")
            else:
                print(f"\n   âš  No pitch logs collected for game_id={game_id}")

            # Circuit breaker ãŒé–‹ã„ã¦ã„ã‚‹å ´åˆã¯å‡¦ç†ã‚’åœæ­¢
            if circuit_breaker_state['is_open']:
                print(f"ğŸš¨ Circuit breaker is open, stopping processing")
                break

        context.close()
        browser.close()
        
    print(f"\nğŸ¯ Step 3 Complete â€” check {OUTPUT_DIR}")
    print(f"ğŸ“Š Final circuit breaker state: {circuit_breaker_state['failures']} failures")

if __name__ == "__main__":
    main()