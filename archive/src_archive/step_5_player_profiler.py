# step_final_npb_scraper_v-perfect-fixed.py (ãƒ•ã‚¡ã‚¤ãƒ«åã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆ)

import os
import re
import time
from datetime import datetime
from urllib.parse import urljoin
import json
import requests
from bs4 import BeautifulSoup
import pandas as pd
import jaconv

# --- è¨­å®š ---
OUTPUT_DIR = "data/player_database_npb"
PLAYERS_JSON_DIR = os.path.join(OUTPUT_DIR, "players")
os.makedirs(PLAYERS_JSON_DIR, exist_ok=True)
BASE_URL = "https://npb.jp"

# (KANA_MAPã¯å¤‰æ›´ãªã—)
def create_kana_map():
    kana_groups = { '0': "ã‚ã„ã†ãˆãŠã‚”", '1': "ã‹ããã‘ã“ãŒããã’ã”", '2': "ã•ã—ã™ã›ãã–ã˜ãšãœã", '3': "ãŸã¡ã¤ã¦ã¨ã ã¢ã¥ã§ã©", '4': "ãªã«ã¬ã­ã®", '5': "ã¯ã²ãµã¸ã»ã°ã³ã¶ã¹ã¼ã±ã´ã·ãºã½", '6': "ã¾ã¿ã‚€ã‚ã‚‚", '7': "ã‚„ã‚†ã‚ˆ", '8': "ã‚‰ã‚Šã‚‹ã‚Œã‚", '9': "ã‚ã‚’ã‚“" }
    kana_map = {}
    for code, chars in kana_groups.items():
        for char in chars: kana_map[char] = code
    return kana_map
KANA_MAP = create_kana_map()

def generate_player_id(info: dict) -> str:
    league_code = info.get('league_code', '0')
    entry_year_code = str(info.get('entry_year', 0))[-3:]
    nationality_code = info.get('nationality_code', '0')
    position_code = info.get('position_code', '0')
    birth_date_obj = info.get('birth_date')
    birth_date_code = birth_date_obj.strftime('%Y%m%d') if birth_date_obj else '00000000'
    name_kana_hira = jaconv.kata2hira(info.get('name_kana', 'ï¼Ÿ'))
    
    # â˜…â˜…â˜…â˜…â˜… ä¿®æ­£ç‚¹â‘  â˜…â˜…â˜…â˜…â˜…
    # èª­ã¿ä»®åãŒãªã„å ´åˆã€IDã®æœ«å°¾ã‚’'?'ã§ã¯ãªã'X'ã«ã™ã‚‹
    initial_code = KANA_MAP.get(name_kana_hira[0], 'X') if name_kana_hira else 'X'
    
    return f"{league_code}{entry_year_code:0>3}{nationality_code}{position_code}{birth_date_code}{initial_code}"

def get_all_player_urls() -> set:
    # (ã“ã®é–¢æ•°ã¯å¤‰æ›´ãªã—)
    print("Fetching all player URLs from NPB.jp (Perfect Gojuon)...")
    player_urls = set()
    kana_romaji_list = ['a', 'i', 'u', 'e', 'o', 'ka', 'ki', 'ku', 'ke', 'ko', 'sa', 'si', 'su', 'se', 'so', 'ta', 'ti', 'tu', 'te', 'to', 'na', 'ni', 'nu', 'ne', 'no', 'ha', 'hi', 'hu', 'he', 'ho', 'ma', 'mi', 'mu', 'me', 'mo', 'ya', 'yu', 'yo', 'ra', 'ri', 'ru', 're', 'ro', 'wa']
    for romaji in kana_romaji_list:
        index_url = f"{BASE_URL}/bis/players/all/index_{romaji}.html"
        try:
            res = requests.get(index_url, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.content, 'html.parser')
            player_list_div = soup.select_one('div.three_column_player')
            if player_list_div:
                for a_tag in player_list_div.select('a'):
                    full_url = urljoin(BASE_URL, a_tag['href'])
                    player_urls.add(full_url)
            print(f"  Successfully fetched URLs from: {index_url}")
            time.sleep(1) 
        except requests.RequestException as e: print(f"Error fetching {index_url}: {e}")
    print(f"Found {len(player_urls)} unique player URLs.")
    return player_urls

def parse_player_page(url: str) -> dict:
    try:
        res = requests.get(url, timeout=10)
        res.raise_for_status()
        html_content = res.content
        soup = BeautifulSoup(html_content, 'html.parser')
        player_data = {'url': url}
        player_data['name'] = soup.select_one('li#pc_v_name').get_text(strip=True)
        player_data['name_kana'] = soup.select_one('li#pc_v_kana').get_text(strip=True) if soup.select_one('li#pc_v_kana') else ''
        bio_table = soup.select_one('section#pc_bio table')
        profile = {}
        if bio_table:
            for row in bio_table.select('tr'):
                header = row.select_one('th').get_text(strip=True)
                value = row.select_one('td').get_text(strip=True)
                profile[header] = value
        player_data['profile'] = profile
        if 'ç”Ÿå¹´æœˆæ—¥' in profile:
            dt_match = re.search(r'(\d+)å¹´(\d+)æœˆ(\d+)æ—¥', profile['ç”Ÿå¹´æœˆæ—¥'])
            if dt_match: player_data['birth_date'] = datetime(int(dt_match.group(1)), int(dt_match.group(2)), int(dt_match.group(3)))
        if 'ãƒ‰ãƒ©ãƒ•ãƒˆ' in profile:
            year_match = re.search(r'(\d{4})å¹´', profile['ãƒ‰ãƒ©ãƒ•ãƒˆ'])
            if year_match: player_data['entry_year'] = int(year_match.group(1))
        player_data['nationality_code'] = '2' if re.search(r'[a-zA-Z\s\.]', player_data.get('name_kana', '')) else '1'
        player_data['position_code'] = '1' if 'æŠ•æ‰‹' in str(soup) else '2'
        stats_dfs = []
        for table_id in ['tablefix_b', 'tablefix_p']:
            try:
                dfs = pd.read_html(html_content, attrs={'id': table_id}, flavor='lxml')
                if dfs:
                    df = dfs[0]
                    # â˜…â˜…â˜…â˜…â˜… ä¿®æ­£ç‚¹â‘¡ â˜…â˜…â˜…â˜…â˜…
                    # SettingWithCopyWarningã‚’å›é¿ã™ã‚‹ãŸã‚ã«.copy()ã‚’è¿½åŠ 
                    df = df[pd.to_numeric(df['å¹´åº¦'], errors='coerce').notna()].copy()
                    df['stats_type'] = 'batting' if table_id == 'tablefix_b' else 'pitching'
                    stats_dfs.append(df)
            except ValueError: continue
        if stats_dfs: player_data['stats_df'] = pd.concat(stats_dfs, ignore_index=True)
        return player_data
    except Exception as e:
        print(f"Error processing {url}: {e}")
        return None

def main():
    player_urls = get_all_player_urls()
    if not player_urls:
        print("No player URLs found. Exiting.")
        return

    player_index_list = []
    new_players_processed = 0 # æ–°è¦å‡¦ç†ã—ãŸé¸æ‰‹æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹å¤‰æ•°

    print("\nChecking for new players and processing...")
    for i, url in enumerate(list(player_urls)):
        # ã¾ãšã¯IDã‚’ä»®ç”Ÿæˆã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†
        # URLã‹ã‚‰é¸æ‰‹ãƒšãƒ¼ã‚¸ID(æ•°å­—)ã‚’æŠœãå‡ºã™ã®ã¯é›£ã—ã„ã®ã§ã€ä¸€åº¦ãƒšãƒ¼ã‚¸ã‚’èª­ã‚“ã§æƒ…å ±ã‚’å¾—ã‚‹å¿…è¦ãŒã‚ã‚‹
        # ãã®ãŸã‚ã€ã“ã“ã§ã¯ç°¡æ˜“çš„ãªãƒã‚§ãƒƒã‚¯ã«ç•™ã‚ã‚‹ã‹ã€ã‚ã‚‹ã„ã¯æ¯å›å…¨ä»¶ãƒã‚§ãƒƒã‚¯ã™ã‚‹å½¢ã«ãªã‚‹
        # ä»Šå›ã¯ã€ã‚ˆã‚Šç¢ºå®Ÿãªã€ŒIDç”Ÿæˆå¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã€ã‚’æ¡ç”¨ã™ã‚‹

        # æ¯å›å…¨é¸æ‰‹ã‚’å‡¦ç†ã™ã‚‹ã®ã§ã¯ãªãã€å·®åˆ†ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹
        # ã“ã®ãƒ«ãƒ¼ãƒ—ã¯URLã®ãƒªã‚¹ãƒˆã‚’å›ã™
        
        # é¸æ‰‹ãƒšãƒ¼ã‚¸ã®æƒ…å ±ã‚’ã¾ãšå–å¾—
        data = parse_player_page(url)
        if not data:
            print(f"  Skipping URL (parse failed): {url}")
            continue

        player_id = generate_player_id(data)
        
        # â˜…â˜…â˜…â˜…â˜… å·®åˆ†æ›´æ–°ã®æ ¸å¿ƒéƒ¨åˆ† â˜…â˜…â˜…â˜…â˜…
        json_filepath = os.path.join(PLAYERS_JSON_DIR, f"{player_id}.json")
        if os.path.exists(json_filepath):
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã€ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹
            if i % 500 == 0: # 500ä»¶ã”ã¨ã«é€²æ—ã‚’è¡¨ç¤º
                 print(f"  Skipping existing player {i+1}/{len(list(player_urls))}: {data.get('name')}")
            continue

        # --- ä»¥ä¸‹ã¯æ–°è¦é¸æ‰‹ã ã£ãŸå ´åˆã®ã¿å®Ÿè¡Œã•ã‚Œã‚‹ ---
        new_players_processed += 1
        print(f"âœ¨ Found new player! Processing {i+1}/{len(list(player_urls))}: {data.get('name')}")
        
        # ç´¢å¼•ãƒªã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        index_record = { 'player_id': player_id, 'name': data.get('name'), 'url': data.get('url') }
        player_index_list.append(index_record)
        
        # å€‹åˆ¥JSONãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        player_json_data = {
            'player_id': player_id,
            'name': data.get('name'),
            'name_kana': data.get('name_kana'),
            'profile': data.get('profile', {}),
            'url': data.get('url')
        }
        
        if 'stats_df' in data and data['stats_df'] is not None:
            stats_records = data['stats_df'].to_dict('records')
            player_json_data['stats'] = stats_records
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(player_json_data, f, indent=4, ensure_ascii=False)
            
        time.sleep(1)

    print("\n--- Update Summary ---")
    print(f"âœ… New players found and processed: {new_players_processed}")

    # æ–°è¦é¸æ‰‹ãŒã„ãŸå ´åˆã®ã¿ã€ç´¢å¼•ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã™ã‚‹
    if player_index_list:
        print("Updating index file...")
        # æ—¢å­˜ã®ç´¢å¼•ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        index_filepath = os.path.join(OUTPUT_DIR, "player_index.csv")
        try:
            existing_df = pd.read_csv(index_filepath)
            new_df = pd.DataFrame(player_index_list)
            # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            updated_df = pd.concat([existing_df, new_df], ignore_index=True)
        except FileNotFoundError:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã‘ã‚Œã°ã€æ–°è¦ä½œæˆ
            updated_df = pd.DataFrame(player_index_list)
        
        # é‡è¤‡ã‚’å‰Šé™¤ã—ã€IDã§ã‚½ãƒ¼ãƒˆã—ã¦ä¿å­˜
        updated_df.drop_duplicates(subset=['player_id'], keep='last', inplace=True)
        updated_df.sort_values('player_id', inplace=True)
        updated_df.to_csv(index_filepath, index=False, encoding="utf-8-sig")
        print(f"âœ… Player index file updated: {index_filepath}")
    else:
        print("No new players to add to the index.")
        
    print("\nğŸ¯ All tasks complete.")
if __name__ == "__main__":
    main()