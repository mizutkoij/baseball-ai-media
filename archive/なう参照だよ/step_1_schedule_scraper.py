# step_1_schedule_scraper.py (æœ€çµ‚å®Ÿè¡Œæ—¥è¨˜éŒ²ãƒ»å·®åˆ†æ›´æ–°ç‰ˆ)

import pandas as pd
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import os
import re
import time
import glob
from urllib.parse import urljoin

# === è¨­å®š ===
BASE_URL = "https://baseball.yahoo.co.jp/npb"
OUTPUT_DIR = "fetch/data/game_info"
LAST_RUN_FILE = os.path.join(OUTPUT_DIR, "last_run_date.txt") # æœ€çµ‚å®Ÿè¡Œæ—¥ã‚’è¨˜éŒ²ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«
os.makedirs(OUTPUT_DIR, exist_ok=True)

# === ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å–å¾—é–‹å§‹æ—¥ï¼ˆåˆå›žå®Ÿè¡Œæ™‚ã®ã¿ä½¿ç”¨ï¼‰ ===
DEFAULT_START_DATE = datetime(2025, 2, 10)

# === 2è»å«ã‚€ gameKindIds ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
GAME_KIND_IDS = "60,61,62,13,11,64,65,66,272"
CENTRAL_TEAMS = {"å·¨äºº", "é˜ªç¥ž", "ä¸­æ—¥", "DeNA", "åºƒå³¶", "ãƒ¤ã‚¯ãƒ«ãƒˆ"}
PACIFIC_TEAMS = {"ãƒ­ãƒƒãƒ†", "ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯", "æ¥½å¤©", "æ—¥æœ¬ãƒãƒ ", "ã‚ªãƒªãƒƒã‚¯ã‚¹", "è¥¿æ­¦"}
VALID_STATUS = {"è©¦åˆçµ‚äº†", "è©¦åˆä¸­æ­¢", "ãƒŽãƒ¼ã‚²ãƒ¼ãƒ "}

def detect_league(home, away):
    if home in CENTRAL_TEAMS and away in CENTRAL_TEAMS: return "ã‚»ãƒ»ãƒªãƒ¼ã‚°"
    if home in PACIFIC_TEAMS and away in PACIFIC_TEAMS: return "ãƒ‘ãƒ»ãƒªãƒ¼ã‚°"
    if ({home, away} & CENTRAL_TEAMS) and ({home, away} & PACIFIC_TEAMS): return "äº¤æµæˆ¦"
    return "ãã®ä»–"

def fetch_daily_schedule(date, game_kind_ids=None):
    url = f"{BASE_URL}/schedule/?date={date:%Y-%m-%d}"
    if game_kind_ids: url += f"&gameKindIds={game_kind_ids}"
    
    try:
        res = requests.get(url, timeout=10)
        res.raise_for_status()
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, "html.parser")
    except requests.RequestException as e:
        print(f"  Error fetching {url}: {e}")
        return []

    results = []
    for section in soup.select("section.bb-score"):
        match_type = section.select_one("h1.bb-score__title").get_text(strip=True) if section.select_one("h1.bb-score__title") else ""
        for item in section.select("li.bb-score__item"):
            status_tag = item.select_one("p.bb-score__link")
            if not status_tag or status_tag.get_text(strip=True) not in VALID_STATUS: continue
            a = item.select_one("a.bb-score__content")
            if not a or 'href' not in a.attrs: continue
            href = a['href']
            m = re.search(r"/game/(\d+)/", href)
            if not m: continue
            game_id = m.group(1)
            home_team = item.select_one('p.bb-score__homeLogo').get_text(strip=True)
            away_team = item.select_one('p.bb-score__awayLogo').get_text(strip=True)
            results.append({
                "è©¦åˆæ—¥": date.strftime("%Y/%m/%d"),
                "é–‹å‚¬åœ°": item.select_one("span.bb-score__venue").get_text(strip=True) if item.select_one("span.bb-score__venue") else None,
                "å¯¾æˆ¦ã‚«ãƒ¼ãƒ‰": f"{home_team} vs {away_team}", "game_id": game_id, "URL": urljoin(BASE_URL, href),
                "è©¦åˆçŠ¶æ…‹": status_tag.get_text(strip=True), "è©¦åˆç¨®åˆ¥": match_type, "ç¨®åˆ¥": detect_league(home_team, away_team),
            })
    return results

# === ãƒ¡ã‚¤ãƒ³å‡¦ç† ===
print("â–¶ Step 1: Scraping game schedules (incremental update)...")

# â˜…â˜…â˜…â˜…â˜… å¤‰æ›´ç‚¹â‘ : æœ€çµ‚å®Ÿè¡Œæ—¥ã‚’èª­ã¿è¾¼ã‚€ â˜…â˜…â˜…â˜…â˜…
try:
    with open(LAST_RUN_FILE, 'r') as f:
        last_run_str = f.read().strip()
        # æœ€çµ‚å®Ÿè¡Œæ—¥ã®ç¿Œæ—¥ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆ
        start_date = datetime.strptime(last_run_str, "%Y-%m-%d") + timedelta(days=1)
    print(f"Found last run date: {last_run_str}. Starting from {start_date.strftime('%Y-%m-%d')}.")
except FileNotFoundError:
    start_date = DEFAULT_START_DATE
    print(f"No last run date found. Starting from default: {start_date.strftime('%Y-%m-%d')}.")

# å–å¾—çµ‚äº†æ—¥ã¯å¸¸ã«ã€Œæ˜¨æ—¥ã€
end_date = datetime.now() - timedelta(days=1)

if start_date > end_date:
    print("No new dates to process. All data is up to date.")
    print("\nðŸŽ¯ Step 1 Complete.")
else:
    seen_ids = set()
    current_date = start_date
    while current_date <= end_date:
        date_str = current_date.strftime("%Y-%m-%d")
        print(f"ðŸ“… Fetching data for: {date_str}")
        daily_games = []
        for kinds in (None, GAME_KIND_IDS):
            games = fetch_daily_schedule(current_date, game_kind_ids=kinds)
            for g in games:
                if g['game_id'] not in seen_ids:
                    daily_games.append(g)
                    seen_ids.add(g['game_id'])
        if daily_games:
            df_day = pd.DataFrame(daily_games)
            df_day.to_csv(os.path.join(OUTPUT_DIR, f"{date_str}.csv"), index=False, encoding="utf-8-sig")
            print(f"  âœ… Saved {len(df_day)} games to {date_str}.csv")
        else:
            print(f"  - No games found for {date_str}.")
        time.sleep(1) 
        current_date += timedelta(days=1)

    # â˜…â˜…â˜…â˜…â˜… å¤‰æ›´ç‚¹â‘¡: æœ€çµ‚å®Ÿè¡Œæ—¥ã‚’è¨˜éŒ²ã™ã‚‹ â˜…â˜…â˜…â˜…â˜…
    with open(LAST_RUN_FILE, 'w') as f:
        f.write(end_date.strftime("%Y-%m-%d"))
    print(f"\nUpdated last run date to: {end_date.strftime('%Y-%m-%d')}")

    # å…¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’1ã¤ã®CSVã«ã¾ã¨ã‚ã‚‹ï¼ˆæ¯Žå›žæ›´æ–°ï¼‰
    print("\nCombining all daily CSVs into a single master file...")
    all_csvs = sorted(glob.glob(os.path.join(OUTPUT_DIR, "*.csv")))
    if all_csvs:
        dfs_to_concat = [pd.read_csv(p) for p in all_csvs if os.path.getsize(p) > 0]
        if dfs_to_concat:
            df_all = pd.concat(dfs_to_concat, ignore_index=True)
            df_all.drop_duplicates(subset=['game_id'], keep='last', inplace=True)
            df_all.to_csv(os.path.join(OUTPUT_DIR, "game_info_all.csv"), index=False, encoding="utf-8-sig")
            print(f"âœ… Combined data saved to game_info_all.csv ({len(df_all)} total games).")
        else:
            print("- No data to combine.")
            
    print("\nðŸŽ¯ Step 1 Complete.")